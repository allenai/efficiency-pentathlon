# For efficiency benchmark
codecarbon
sacrebleu>=1.5.0
sentencepiece


pyyaml>=5.4.1

# catwalk dependencies
ai2-tango[torch,transformers,fairscale,beaker,wandb]>=1.1

torchmetrics==0.11.0
more_itertools
spacy>=3.0.0
wget
datasets>=2.1.0
accelerate
bettermap

# For the P3 datasets, which we get from huggingface datasets
protobuf<=3.20

# For lm-eval
scikit-learn>=0.24.1   # Eleuther uses this for metrics. Can we replace it with torchmetrics?
pycountry>=20.7.3
rouge-score>=0.0.4  # Can we replace this with torchmetrics?
# The Eleuther test harness depends on these even at runtime, but does not declare them.
mypy_extensions
pytest

# For promptsource
jinja2
